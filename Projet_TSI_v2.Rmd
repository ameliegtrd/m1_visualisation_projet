---
title: "Projet - Traitement du Signal et des Images"
author:
- name: GUILLOTEL Lou-Anne
- name: GOUTARD Amélie
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
# Option de chunk
knitr::opts_chunk$set(
 fig.align="center"
)
```

</br>

Chargement des packages : 
```{r, message=FALSE, warning=FALSE} 
library(tidyverse)
library(janitor)
library(fda)
library(wavethresh)
library(tidyfun)
library(geobr)
library(sf)
```

</br>

# Présentation du jeu de données

Le jeu de données utilisé pour ce projet s'intitule "Forest Fires in Brazil". Il est issu du site Kaggle (https://www.kaggle.com/datasets/gustavomodelli/forest-fires-in-brazil) et s'inscrit dans le contexte suivant. 

Les feux de forêt sont un problème sérieux pour la préservation des forêts tropicales. Comprendre la fréquence des feux de forêt dans une série chronologique peut aider à prendre des mesures pour les prévenir.
Le Brésil possède la plus grande forêt tropicale de la planète, à savoir la forêt amazonienne.

Ce jeu de données rapporte le nombre d'incendies de forêt au Brésil pour chacun des 27 états du Brésil. La série comprend une période d'environ 10 ans (1998 à 2017). Les données ont été obtenues sur le site officiel du gouvernement brésilien (http://dados.gov.br/dataset/sistema-nacional-de-informacoes-florestais-snif). 

Grâce à ces données, il est possible d'évaluer l'évolution des incendies au fil des années ainsi que les régions où ils se sont concentrés.
L'Amazonie légale comprend les états d'Acre, d'Amapá, de Pará, d'Amazonas, de Rondonia, de Roraima, et une partie du Mato Grosso, du Tocantins et du Maranhão.

</br>

# Résumé du pré-traitement opéré

Nous avons apporté plusieurs modifications à ce jeu de données. 

Premièrement, nous avons constaté qu'il n'y avait pas un unique nombre d'incendies de forêt renseigné par couple (MonthYear, state) ; or cela devrait être le cas. Pour l'état Alagoas, cela était anecdotique : il y avait seulement un doublon, et les nombres de feux renseignés étaient bien identiques. Nous avons simplement supprimé une de ces deux lignes. Toutefois, il y avait trois états pour lesquels il existait plusieurs nombres d'incendies différents pour un même MonthYear. Suite à la lecture de forums, ce problème semblerait s'expliquer par le fait que plusieurs états pourtant différents ont été nommés de la même manière lors de la construction de ce jeu de données :
  -Rio = Rio de Janeiro
  -Rio = Rio Grande do Norte
  -Rio = Rio Grande do Sul
  -Mato Grosso = Mato Grosso
  -Mato Grosso = Mato Grosso do Sul
  -Paraiba = Paraíba
  -Paraiba = Paraná
Nous nous sommes appuyées sur le travail d'un membre de Kaggle pour retrouver les bons états.

Finalement, nous disposons de **239 temps de mesure** (variables) pour chacun des **27 états** (individus). 

\vspace{0.3cm}

Par ailleurs, nous ne comprenions pas pourquoi il y avait des décimales dans la variable "number", alors qu'il s'agit du nombre d'incendies. Suite à la lecture de forums, nous comprenons qu'au Brésil, le point "." représente le séparateur de milliers. Nous avons donc modifié cela. 

\vspace{0.3cm}

Ensuite, nous avons fait le choix de ne pas conserver comme mesure d'intérêt le nombre de feux de forêts. En effet, nous avons estimé que cette quantité était forcément en partie proportionnelle avec la taille de l'état, et donc que les résultats seraient biaisés. Nous avons donc divisié le nombre de feu de forêts par la superficie de chaque état en km2, et avons multiplié cette quantité par 100 000. Notre mesure d'intérêt est donc finalement le **nombre de feux de forêt pour 100 000 km2**. 

\vspace{0.3cm}

Finalement, nous avons ajouté à notre jeu de données une nouvelle variable, à savoir le climat de chacun des états. Cela nous semble en effet pertinent pour compléter notre analyse et aider à l'interprétation. 


Chargement du jeu de données modifié : 
```{r}
load("data/amazon_deltalite_mod.RData")
summary(amazon)
```


# Premières représentations (statistiques descriptives)

Nous commençons par créer une nouvelle mise en forme de notre jeu de données qui sera utile pour la suite. Elle correspond en effet aux formats que nous avons étudiés en cours, avec les individus (state) en colonnes et les variables (YearMonth) en lignes.

```{r}
amazon_pivot <- amazon %>% 
  dplyr::select(state, YearMonth, fires_km2) %>% 
  pivot_wider(names_from = state, values_from = fires_km2) %>% 
  as.data.frame()
row.names(amazon_pivot) <- amazon_pivot[,1]
amazon_pivot <- amazon_pivot[,-1]
amazon_pivot <- as.matrix(amazon_pivot)
```

\vspace{0.4cm}


Regardons où sont situés les états et quels sont leur type de climat respectifs :
```{r}
# on récupère les données géographique des états du brésil 
states_coord <- read_state(
  year=2019, 
  showProgress = FALSE
  ) %>% 
  arrange(name_state)

head(states_coord)

# on récupère les climats associés à chaque état
amazon_climate <- amazon %>% 
  distinct(state, .keep_all = TRUE) %>% 
  dplyr::select(state, area, climate) %>% 
  mutate(
    couleur = ifelse(
      climate == "Equatorial", "#1522B1", 
      ifelse(climate == "Tropical humide", "#4655FD", 
             ifelse(climate == "Tropical de savane", "#64B5F8", 
                    ifelse(climate =="Semi-aride", "#FF9A00", 
                           ifelse(climate == "Altitude Tropicale", "#A1EDA8", 
                                  ifelse(climate == "Subtropical", "#C0F452", "")))))
    )
  )

# on concatène
states_coord <- cbind(states_coord, amazon_climate)
states_coord <- states_coord %>% dplyr::select(-name_state)

# on affiche
ggplot() +
  geom_sf(data = states_coord, fill=states_coord$couleur, color = "#ffffff", size=.15, show.legend = "line") +
  labs(subtitle="Etats du Brésil", size=10) +
  # geom_text(
  #   data = states_coord,
  #   aes(X, Y, label = state)
  # ) +
  theme(
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    # legend.title = "Climat",
    # legend.position="right"
  )
```
Le Brésil ne possède pas les mêmes saisons qu'en France. En effet, l'été dure de décembre à mars et l'hiver de juin à décembre. Dans un pays d’une telle étendue, il existe à la même période de l´année d´importantes différences climatiques selon les régions.  

A titre informatif, le *climat équatorial* est un climat chaud et humide toute l'année. Le *climat tropical* possède une saison sèche plus ou moins longue. On distingue le climat *tropical de savane* qui possède une saison sèche de mai à novembre et une humide de décembre à avril du climat *tropical humide* où les pluies sont plus faibles de juin à août mais plus importantes le reste de l'année. L'*altitude tropicale*, quant à elle, se caractérise par des températures douces et modérées avec des précipitations régulières. 
Pour le climat *subtropical*, la chaleur et l'humidité sont nettes en été mais l'hiver peut connaître des coups de froid. 
Enfin, le climat *semi-aride* se caractérise par une forte chaleur et une pluviosité plutôt faible et très irrégulière. On ne distingue pas de saisons sèche et humide.  

ME RESTE A TERMINER pour la carte : rajouter le nom des états

Représentons le nombre d'incendies par état, avec une couleur par type de climat :

```{r}
climates <- amazon %>% 
  distinct(state, climate) %>% 
  pull(climate)
matplot(1:239, amazon_pivot, type="l", main="Nombre d'incendies par état au Brésil entre 1998 et 2017", xlab="Temps de mesure", ylab="Nombre d'incendies (pour 100 000 km2)", col=as.factor(climates))
legend("topleft", levels(as.factor(climates)), col=1:7, lty=1, cex=0.5)
```

Les profils sont relativement semblables entre les états et semblent se répéter chaque année (20 pics pour 20 années), avec des pics et des creux qui apparaissent aux mêmes moments de l'année pour chaque état. Nous pouvons supposer que les incendies sont plus importants pendant une période plus sèche. Or bien que le climat soit très variable d'un état à l'autre au Brésil, il existe une saison sèche qui s'étend de juin à octobre. Vérifions-le :

```{r}
amazon %>% 
  group_by(as.factor(month)) %>% 
  summarise(fires_km2_mean = mean(fires_km2)) %>% 
  rename(month = "as.factor(month)")
```

Effectivement, nous voyons que les mois pour lesquels le nombre moyen d'incendies pour 100 000km² dépasse les 300 sont août, septembre et octobre. 

Notons qu'il y a tout de même des différences importantes du nombre d'incendies moyen selon les états, liées à des climats très variés.

```{r}
amazon %>% 
  group_by(state) %>% 
  summarise(fires_km2_mean = mean(fires_km2),
            fires_km2_var = var(fires_km2)) %>% 
  arrange(-fires_km2_mean)
```

Enfin, le graphique suggère une différence importante du nombre d'incendies selon le type de climat. Les états ayant un climat "Tropical de savane" semblent avoir des pics très importants, tandis que le nombre de feux de forêts des états au climat subtropical ou bien équatorial semble demeurer beaucoup plus bas et stable.  Vérifions-le numériquement :

```{r}
amazon %>% 
  group_by(climate) %>% 
  summarise(fires_km2_mean = mean(fires_km2),
            fires_km2_var = var(fires_km2)) %>% 
  arrange(-fires_km2_mean)
```


# Régression non-paramétrique

Nos données observées sont supposées être les mesures bruitées de signaux continus, existant en tout point de temps. Elles sont évaluées sur une grille de discrétisation qui correspond dans notre cas aux couples (année, mois) entre janvier 1998 et septembre 2017, pour chaque individu. L'objectif ici est de constituer les signaux individuels sous forme fonctionnelle, c'est-à-dire un objet évaluable en tout point de temps. 

Plus précisément, nous nous appuyons sur le lissage pour retrouver la fonction $f_i(t)$ à partir des observations bruitées. Nous ne supposons pas de forme particulière pour $f(t)$, nous plaçant ainsi dans le cadre de la régression non-paramétrique.

## Lissage par moindres carrés

Nous voulons trouver $\hat{f}$ qui minimise le critère des moindres carrés. Pour ne pas être dans un problème en dimension infinie, nous cherchons $\hat{f}$ comme combinaison linéaire de fonctions de bases pour une base donnée.

### Avec une base de Fourier

Pour commencer, nous nous concentrons sur la premier état (ACRE). Nous représentons la courbe de ses incendies de forêt : 
```{r}
i=1
y = amazon_pivot[,i]
plot(y, type="l" ,main=paste("Nombre d'incendies de forêt", colnames(amazon_pivot)[i], sep=" "), xlab="Temps de mesure", ylab="Nombre d'incendies")
```

Nous regardons ce que cela donne avec une base de Fourier, en faisant varier le nombre de fonctions de bases. 

```{r}
nb_fonctions = c(9, 17, 25, 41)
par(mfrow=c(2, 2))
for(n in nb_fonctions){
  fbasis <- create.fourier.basis(rangeval=c(1, 239), nbasis=n)
  c_hat = Data2fd(1:239, y, basisobj = fbasis)
  fhat = eval.fd(1:239, c_hat)
  plot(y, pch=20, cex=0.5, main=paste("nombre de fonctions de base = ", n, sep=""), xlab="Temps de mesure", ylab="Nombre d'incendies")
  lines(fhat, col=4, lwd=2)
}
```

Nous remarquons qu'un nombre de fonctions de base très important est nécessaire pour obtenir une qualité d'ajustement relativement satisfaisante ; et encore, les grands pics sont très mal captés. 

Ces mauvais résultats ne sont pas étonnants car les bases de Fourier sont adaptées à l'étude des signaux réguliers, ce qui n'est pas du tout le cas de nos données. 

Regardons ce que cela donne avec une base de B-Splines. 

</br>

### Avec une base de B-Splines

Nous commençons par utiliser une base de B-splines pour le premier état toujours.

- En faisant varier l'ordre de la base de B-Splines avec 30 noeuds équirépartis : 
```{r}
norder = c(2, 3, 4, 20)
par(mfrow=c(2, 2))
for(d in norder){
  splbasis <- create.bspline.basis(rangeval=c(1, 239), norder=d, breaks=seq(1, 239, length=30))
  c_hat <- Data2fd(1:239, y, basisobj = splbasis)
  fhat <- eval.fd(1:239, c_hat)
  plot(y, pch=20, cex=0.5, main=paste("ordre = ", d, sep=""), xlab="Temps de mesure", ylab="Nombre d'incendies")
  lines(fhat,col=4,lwd=2)
}
```

Nous constatons que même lorsque nous allons jusqu'à l'ordre 20, qui est l'ordre maximal possible, l'ajustement est très mauvais.  

- En faisant varier le nombre de noeuds de la base de B-Splines avec un ordre de 4 : 
```{r}
noeuds = c(2, 4, 8, 50)
par(mfrow=c(2, 2))
for(l in noeuds){
  splbasis = create.bspline.basis(rangeval=c(1, 239), norder=4, breaks=seq(1, 239, length=l))
  c_hat = Data2fd(1:239, y,basisobj = splbasis)
  fhat = eval.fd(1:239, c_hat)
  plot(y, pch=20,cex=0.5, main=paste("nombre de noeuds = ", l, sep=""), xlab="Temps de mesure", ylab="Nombre d'incendies")
  lines(fhat, col=4, lwd=2)
}
```

Rappelons que le nombre de noeuds choisi joue sur le fameux compromis biais / variance. Plus précisément, trop peu de noeuds donne une approximation biaisée (pas assez proche des données observées - underfitting) tandis que trop de noeuds donne une reconstruction "ondulante", trop flexible (overfitting). 
Mais ici, nous observons qu'il faut choisir un nombre de noeuds très important pour que l'ajustement soit relativement satisfaisant, et même avec cela les pics ne sont pas bien captés... 

</br>

Ces mauvais résultats ne sont pas étonnants : la base de B-splines est adaptée à l'étude des signaux réguliers et non périodiques, or nos données sont totalement périodiques : le même motif se répète chaque année avec un pic important entre août et octobre et un nombre d'incendies beaucoup plus faible le reste du temps.

</br>

Nous pouvons tout de même essayer d'améliorer ces résultats. Afin de réaliser un compromis biais-variance, nous pouvons utiliser le critère des moindres carrés pénalisés, qui consiste à rajouter un terme contrôlant la rugosité de la fonction, afin d'éviter que celle-ci soit trop oscillante. La quantité $\lambda$ est un hyperparamètre à calibrer réalisant le compromis entre ajustement et rugosité.


Nous reprenons la courbe du nombre d'incendies observé de premier état (ACRE). Nous effectuons le lissage par moindres carrés pénalisés de cette courbe avec des noeuds placés en les points de mesure ($t_j$) et en sélectionnant le paramètre $\lambda$ minimisant le critère des moindres carrés généralisés.

```{r}
splbasis239 <- create.bspline.basis(c(1, 239), norder=4, breaks=1:239)
gcv = 1:21
for (i in 1:21){
  lambda <- exp(i-10)
  fdparTemp <- fdPar(splbasis239, Lfdobj = 2, lambda=lambda)
  smoothdata <- smooth.basis(1:239, y, fdParobj = fdparTemp) 
  gcv[i] = smoothdata$gcv  
}
plot(gcv)
which.min(gcv)
```

Le minimum se trouve en i=1. 

Nous représentons sur un même graphe les observations et la fonction estimée reconstruite $\hat{f}$.
 
```{r}
lambda <- exp(which.min(gcv)-10)
fdparTemp <- fdPar(splbasis239, Lfdobj = 2, lambda=lambda)
smoothdata <- smooth.basis(1:239, y, fdParobj = fdparTemp)
plotfit.fd(y, 1:239, smoothdata$fd, pch=20, cex=0.5, main = "Représentation des données brutes et des données ajustés \n pour l'état ACRE", xlab="Temps de mesure", ylab="Nombre d'incendies")
```

Les pics semblent mieux captés mais lorsque l'on regarde la valeur des résidus (0.56), ils sont vraiment très importants par rapport aux exemples étudiés en cours...

\vspace{0.4cm}

Sur plusieurs courbes à la fois désormais, nous regardons la moyenne des critères gcv sur tous les individus pour chaque valeur de $\lambda$. Nous cherchons à calibrer $\lambda$ pour l'ensemble des individus.

```{r}
splbasis239 <- create.bspline.basis(c(1, 239), norder=4, breaks=1:239)
gcv = 1:21
for (i in 1:21){
  lambda <- exp(i-10)
  fdparTemp <- fdPar(splbasis239, Lfdobj = 2,lambda=lambda)
  smoothdata <- smooth.basis(1:239, amazon_pivot, fdParobj = fdparTemp) # au lieu de donner une seule courbe, on donne la matrice de toutes les courbes observées
  gcv[i] = mean(smoothdata$gcv) # moyenne pour trouver le minimum
}
plot(gcv)
which.min(gcv)
```

Cette fois-ci, le minimum est obtenu en i=7. Nous pouvons effectuer de nouveau le lissage par moindres carrés pénalisés avec cette valeur de $\lambda$ et comparer les données brutes et les données lissées.

```{r}
lambda <- exp(which.min(gcv)-10)
fdparTemp <- fdPar(splbasis239, Lfdobj = 2, lambda=lambda)
smoothdata_Bsplines239_MCOpen <- smooth.basis(1:239, amazon_pivot, fdParobj = fdparTemp)
```

```{r}
fhatsmooth_Bsplines239_MCOpen <- eval.fd(1:239, smoothdata_Bsplines239_MCOpen$fd)
par(mfrow=c(1, 2))
matplot(1:239, amazon_pivot, type="l", lty=1, ylab="", xlab="", main="Données brutes")
matplot(1:239, fhatsmooth_Bsplines239_MCOpen, type="l", lty=1, ylab="", xlab="", main="Données lissées")
```

Nous sommes ainsi très (trop ?) proches de nos données de départ. Ce n'est pas étonnant car nous avons pris autant de noeuds que de points de mesure, ce qui entraîne un risque de surajustement. 

Nous pouvons représenter les résidus :
```{r}
matplot(1:239, fhatsmooth_Bsplines239_MCOpen - amazon_pivot, lty=1, type="l", xlab="Temps de mesure", ylab="Résidus")
```

Nous constatons qu'il reste des résidus importants au niveau des pics de nos signaux. 

\vspace{0.4cm}

Essayons de considérer une base plus petite (avec moins de noeuds).

D'abord sur un individu :

```{r}
splbasis239 <- create.bspline.basis(c(1,239), norder=4, breaks=seq(1, 239, 30)) # on prend une base plus petite, avec 30 noeuds équirépartis entre 1 et 239 (alors qu'auparavant il y avait autant de noeuds que de points de mesure)
gcv = 1:21
for (i in 1:21){
  lambda <- exp(i-10)
  fdparTemp <- fdPar(splbasis239, Lfdobj = 2,lambda=lambda)
  smoothdata <- smooth.basis(1:239, y, fdParobj = fdparTemp)
  gcv[i] <- smoothdata$gcv
}
plot(gcv)
which.min(gcv)
```

Cette fois-ci, le minimum est à 21 (on contraint donc beaucoup l'estimateur). Si nous réajustons sur la première courbe, voilà ce que nous obtenons :

```{r}
lambda <- exp(which.min(gcv)-10)
fdparTemp <- fdPar(splbasis239, Lfdobj = 2, lambda=lambda)
smoothdata <- smooth.basis(1:239, y, fdParobj = fdparTemp)
plotfit.fd(y, 1:239, smoothdata$fd, pch=20, cex=0.5, main = "Représentation des données brutes et des données ajustés \n pour l'état ACRE", xlab="Temps de mesure", ylab="Nombre d'incendies")
```

La forme de la courbe ajustée ne suit pas du tout le motif de notre signal, les résidus explosent, cela est très mauvais !

Regardons tout de même ce que cela donne avec tous les signaux :
```{r}
lambda <- exp(which.min(gcv)-10)
fdparTemp <- fdPar(splbasis239, Lfdobj = 2, lambda=lambda)
smoothdata <- smooth.basis(1:239, amazon_pivot, fdParobj = fdparTemp)
```

```{r}
fhatsmooth <- eval.fd(1:239, smoothdata$fd)
par(mfrow=c(1, 2))
matplot(1:239, amazon_pivot, type="l", lty=1, ylab="", xlab="", main="Données brutes")
matplot(1:239, fhatsmooth, type="l", lty=1, ylab="", xlab="", main="Données lissées")
```

Cette base ainsi choisie n'est donc pas du tout adaptée à la forme de nos données : elle sous-ajuste totalement.  

\vspace{0.3cm}

Nous allons donc voir ce que donnent les résultats avec les ondelettes, qui combinent les avantages des deux types de bases précédentes (localisation en fréquence et localisation en temps) et devraient être mieux adaptées à nos données.


### Avec une base d'ondelettes

#### Lissage d'une trajectoire 

Le problème ici est que la longueur des courbes du nombre d'incendies n'est pas de la forme $2^J$. 
Une solution possible est de répéter par symétrie la fin du signal jusqu'à la puissance de 2 suivante. Pour la courbe de température de longueur 239, on symétrise le signal à la fin pour atteindre une longueur de 256 = $2^8$.
On se ramènera à la fin au signal de départ de taille 239.

```{r}
ysym <- c(y, rev(tail(y, n=256-length(y))))
plot(ysym, type="l", main="Signal symétrisé", ylab="Nombre d'incendies", xlab="Temps de mesure")
abline(v=239, col='red', lty=2)
```

Nous décomposons le signal observé dans une base d'ondelettes avec d = 8 moments nuls. Notons que nous utilisons "DaubLeAsym" car nous souhaitons des ondelettes de Daubechies "moins asymétriques" qu'avec "DaubExPhase" qui est plutôt approprié pour des ondelettes à phase extrême.

```{r}
coefs <- wd(ysym, filter.number = 8, family = "DaubLeAsymm") 
plot(coefs, scaling="by.level")
sum(coefs$D!=0) # nombre de coefficients d'ondelettes non nuls
```

C'est la représentation de tous les coefficients, le niveau 7 était le niveau le plus fin. Pour l'instant, aucun coefficient de détails n'est nul.

Effectuons à présent le seuillage des coefficients d'ondelettes.

Seuillage doux :
```{r}
dsoft <- threshold(coefs, type="soft", policy="universal")
plot(dsoft, scaling="by.level")
```

Seuillage dur :
```{r}
dhard <- threshold(coefs, type="hard", policy="universal")
plot(dhard, scaling="by.level")
```

```{r}
sum(dsoft$D!=0)
sum(dhard$D!=0)
```

Dans les deux cas, il ne reste plus que 28 coefficients non-nuls sur les 255 (compression importante). 

Nous avons également effectué différents tests pour d moments nuls variant de 4 à 10 (qui sont les valeurs extrêmes pour la famille "DaubLeAsymm"). Voici les résultats que nous avons obtenus :  
- d =  4 : 42 coefficients non nuls  
- d =  5 : 54 coefficients non nuls  
- d =  6 : 35 coefficients non nuls  
- d =  7 : 41 coefficients non nuls  
- d =  8 : 28 coefficients non nuls  
- d =  9 : 31 coefficients non nuls  
- d = 10 : 25 coefficients non nuls 
Plus on a de coefficients, meilleur est l'ajustement (logique) mais quel est le bon compromis ?  

\vspace{0.3cm}

Observons à présent les reconstructions comparées au vrai signal :
```{r}
fsoft = wr(dsoft)
fhard = wr(dhard)
plot(y, pch=20, cex=0.5, xlab="Temps de mesure", ylab="Nombre d'incendies") 
lines(fsoft[1:239], col=2) # apres reconstruction, on ne conserve que les points de mesures d'origine
lines(fhard[1:239], col=4)
legend("bottomleft", c("soft", "hard"), col=c(2,4), lty=1) 
```

Ainsi, la courbe obtenue grâce au seuillage dur semble plus cohérente vis-à-vis de nos données : elle retranscrit mieux les pics que le seuillage doux. Toutefois, notons que les données lissées ont des valeurs négatives, ce qui est impossible en raison de la nature de nos données (comptage). Il serait alors pertinent de disposer d'un moyen de contraindre les valeurs de nos données lissées.

\vspace{0.3cm}

#### Lissage de toutes les trajectoires  

Nous allons comparer le signal brut et lissé avec une base d'ondelettes à 8 moments nuls et à seuillage hard.

```{r}
par(mfrow=c(1, 2))
# signal brut
matplot(1:239, amazon_pivot, type="l", xlab="Temps de mesure", ylab="Nombre d'incendies", main="Données brutes", col=as.factor(climates))
# signal avec base d'ondelette
i = 1
d = 8 # moments nuls
y = amazon_pivot[,i]
ysym <- c(y, rev(tail(y, n=256-length(y))))
coefs <- wd(ysym, filter.number = d, family = "DaubLeAsymm") 
dhard <- threshold(coefs, type="hard", policy="universal")
fhard <- wr(dhard)
plot(fhard[1:239], pch=20, cex=0.5, main="Bases ondelettes", xlab="Temps de mesure", ylab = "Nombre d'incendies", ylim=c(0,7500)) # créer une colonne de couleurs associée aux climats
j = 1
for(i in 2:ncol(amazon_pivot)){
  y = amazon_pivot[,i]
  ysym <- c(y, rev(tail(y, n=256-length(y))))
  coefs <- wd(ysym, filter.number = d, family = "DaubLeAsymm") 
  dhard <- threshold(coefs, type="hard", policy="universal")
  fhard <- wr(dhard)
  lines(fhard[1:239], col = j)
  j = j + 1
}
# legend("topleft", levels(as.factor(climates)), col=1:7, lty=1, cex=0.5)
```

\vspace{0.4cm}

*Choix de la base*

En résumé, nous avons constaté que la base qui semblait donner le meilleur compromis entre biais et variance était celle des ondelettes. La difficulté est que le package `wavethresh`, utilisé pour construire la base d'ondelettes, ne permet pas de récupérer d'objet fonctionnel, nécessaire pour la suite de notre travail avec l'analyse exploratoire. 

\vspace{0.4cm}

Aussi, nous faisons donc le choix de finalement conserver la base de B-Splines avec un ordre de 4 et  autant de noeuds que de points de mesure (239). Le lissage retenu est donc celui réalisé par moindres carrés pénalisés avec le paramètre $\lambda$ = 7. 

Il est vrai que cela conduit à un surajustement, et que l'objectif de réduction de dimension n'est pas rempli. En revanche, ce choix se justifie grâce au théorème de Green & Silverman (1994) indiquant que le $\hat{f}$ qui minimise le critère es moindres carrés pénalisé est unique et est nécessairement une spline cubique dont les noeuds sont les points $(t_j)_{j=1, ...,m}$. 

Surtout, l'important est d'obtenir un lissage pertinent, qui soit utilisable pour la suite de notre projet, à savoir la statistique exploratoire. 

\vspace{0.4cm}


# Statistique exploratoire

## Moyenne et variance 

Nous commençons par représenter les données lissées par moindres carrés pénalisés (base de B-splines avec 20 noeuds equirépartis), que l'on superpose par la moyenne empirique. Nous évaluons également la variabilité (ponctuelle) de cette moyenne en représentant $\bar{X}_n(t) \pm 2\sqrt{S^2_n(t)}$. Notons que cela ressemble à un intervalle de confiance mais n'en est pas vraiment un puisque ce n'est pas basé sur une loi. 

```{r}
fires_km2.fd.mean <- mean.fd(smoothdata_Bsplines239_MCOpen$fd) # moyenne fonctionnelle
fires_km2.fd.sd <- sd.fd(smoothdata_Bsplines239_MCOpen$fd) # écart-type fonctionnel
matplot(1:239, fhatsmooth_Bsplines239_MCOpen, type="l", col='grey', main = "Représentation des données lissées \n Moyenne empirique et sa variabilité", xlab="Temps de mesure", ylab="Nombre d'incendies")
lines(fires_km2.fd.mean, lwd=2)
lines(fires_km2.fd.mean-2*fires_km2.fd.sd, col=4, lwd=2, lty=2)
lines(fires_km2.fd.mean+2*fires_km2.fd.sd, col=4, lwd=2, lty=2)
```

On observe une variabilité autour de la moyenne empirique plus importante durant la période sèche. En effet, c'est durant cette période que le nombre d'incendies augmente mais certains états sont plus touchés que d'autres. De ce fait, les écarts entre états se creusent durant cette période.  


## Surface de covariance et corrélation 

Représentons désormais les dépendances des observations en différentes valeurs de temps.

### Covariance

```{r}
fires_km2.fd.cov <- var.fd(smoothdata_Bsplines239_MCOpen$fd)
surfcov = eval.bifd(seq(1, 239, length=30), seq(1, 239, length=30), fires_km2.fd.cov)
```

Pour le tracé d'une surface, il est préférable de choisir un nombre modéré de points dans la grille d'évaluation (la représentation est rapidement illisible dans le cas contraire). Ici, nous choisissons arbitrairement 30 points répartis entre 1 et 239.

```{r}
persp(surfcov, col="gray", theta=30, xlab="Temps de mesure", ylab="Temps de mesure", zlab="Covariance")
```
L'interprétation ici est plus difficile qu'avec les données CanadianWeather puisque nous avons des données sur plusieurs années, donc nécéssairement plus de pics. 

```{r}
contour(surfcov)
```
Idem pour cette interprétation ...


```{r}
filled.contour(surfcov)
```
On observe ici aussi une variabilité plus forte pour chaque année sur la période sèche. Enfin, soulignons que les covariances sont toutes positives, indiquant une certaine homogénéité sur les états brésiliens. En effet, si le nombre d'incendies peut différer de beaucoup durant la période sèche entre les états, tous suivent plus ou moins le même schéma (nombre d'incendie faible entre janvier-mars puis forte hausse durant la période sèche et forte descente jusqu'à décembre). 


### Corrélation 

Nous procédons de la même façon pour la corrélation (covariance normalisée par les variances).
```{r}
fires_km2.fd.cor <- cor.fd(seq(1, 239, length=30), smoothdata_Bsplines239_MCOpen$fd)
```
INTERPRETATION

```{r}
persp(fires_km2.fd.cor, col="gray", theta=90, phi=40, xlab="Temps de mesure", ylab="Temps de mesure", zlab="Corrélation")
```
INTERPRETATION

```{r}
filled.contour(fires_km2.fd.cor)
```

Là encore, les conclusions restent similaires en observant les corrélations (covariances normalisées par les variances). Nous remarquons bien que la diagonale est remplie de 1 (un temps de mesure est parfaitement corrélé avec lui-même). 
Difficile d'aussi bien interpréter qu'avec Canadian ...
En revanche, corrélations négatives


## Analyse en Composantes Principales Fonctionnelle

```{r}
fires_km2ACPF <- pca.fd(smoothdata_Bsplines239_MCOpen$fd, nharm = 4, centerfns = TRUE)
fires_km2ACPF$varprop
cumsum(fires_km2ACPF$varprop)
```

- Représentons en premier lieu les composantes principales obtenues
 
```{r}
plot(fires_km2ACPF$harmonics)
```

Ou, pour une interprétation plus aisée, la moyenne augmentée ou diminuée des premières fonctions propres.

On peut utiliser la fonction `plot.pca.fd` pour cette représentation.

```{r}
# plot.pca.fd(fires_km2ACPF)
fires_km2ACPF$varprop
```

 - Représentation des individus (les états) dans le premier plan factoriel.
 
Les scores des individus sont stockés dans la matrice `scores` de `fires_km2ACPF`

```{r}
head(fires_km2ACPF$scores)
plot(fires_km2ACPF$scores[,1], fires_km2ACPF$scores[,2], pch=20, xlab="FPC 1", ylab="FPC 2", type="n")
text(fires_km2ACPF$scores[,1], fires_km2ACPF$scores[,2])
# , labels=climates, cex=0.7, col = as.numeric(as.factor(climates)))
head(fires_km2ACPF$scores)
plot(fires_km2ACPF$scores[,1], fires_km2ACPF$scores[,2], pch=20, xlab="FPC 1", ylab="FPC 2", type="n", xlim=c(-8000, 24000))
text(fires_km2ACPF$scores[,1], fires_km2ACPF$scores[,2], labels=unique(amazon$state), cex=0.7, col = as.numeric(as.factor(climates)))
```

On remarque que 3 états se distinguent particulièrement des autres le long du premier axe : MATO GROSSO DO SUL, PARÁ et MARANHÃO. Ce n'est pas étonnant étant donné que ce sont ceux qui ont et en moyenne le plus d'incendies de forêts entre 1998 et 2017. 
En revanche, MATO GROSSO DO SUL et PARÁ sont à l'opposé par rapport au deuxième axe. 

Voir ce qui les différencie pour essayer de mieux comprendre ce que l'axe 2 montre ?? 
A MODIFIER